{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import fire\n",
    "import gradio as gr\n",
    "import torch\n",
    "import transformers\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, LlamaForCausalLM, LlamaTokenizer\n",
    "sys.path.append('..')\n",
    "from utils.callbacks import Iteratorize, Stream\n",
    "from utils.prompter import Prompter\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "try:\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "except:  # noqa: E722\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = 'togethercomputer/RedPajama-INCITE-Base-3B-v1'\n",
    "lora_weights = \"../lora-redpajama-alpaca-3B\"\n",
    "load_8bit = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GPTNeoXForCausalLM(\n",
      "      (gpt_neox): GPTNeoXModel(\n",
      "        (embed_in): Embedding(50432, 2560)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x GPTNeoXLayer(\n",
      "            (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "            (post_attention_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "            (attention): GPTNeoXAttention(\n",
      "              (rotary_emb): RotaryEmbedding()\n",
      "              (query_key_value): Linear(\n",
      "                in_features=2560, out_features=7680, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=7680, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "            )\n",
      "            (mlp): GPTNeoXMLP(\n",
      "              (dense_h_to_4h): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "              (dense_4h_to_h): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "              (act): GELUActivation()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (embed_out): Linear(in_features=2560, out_features=50432, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    load_in_8bit=load_8bit,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    lora_weights,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "print(model.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "What is this Alpaca thing?\n",
      "Alpacas are a type of camelid, a family of animals that includes llamas, guanacos, vicunas, and guarumas. They are native to South America, and are related to the llama. Alpacas are smaller than llamas, and have a coat that is soft and fluffy. They are also known for their long, silky fur.\n",
      "Alpacas are a type of camelid, a family of animals that includes llamas, guanacos, vicunas, and guarumas. They are native to South America, and are related to the llama. Alpacas are smaller than llamas, and have a coat that is soft and fluffy. They are also known for their long, silky fur. Alpacas are a type of camelid, a family of animals that includes llamas, guanacos, vicunas, and guarumas. They are native to South America, and are related to the llama. Alpacas are smaller than llamas, and have a coat that is soft and fluffy. They are also known for their long, silky fur. Alpacas are a type of camel\n"
     ]
    }
   ],
   "source": [
    "input = \"What is this Alpaca thing?\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "input_ids = tokenizer(input, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "greedy_output  = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    # generation_config=generation_config,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    max_new_tokens=250,\n",
    ")\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0][0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
